import pandas as pd
import numpy as np
import os
import pickle
import random
from collections import defaultdict
from datetime import datetime

# ---------------- CONFIG ----------------
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(BASE_DIR, 'data')
OUTPUT_FILE = os.path.join(BASE_DIR, 'processed_sim_data.pkl')

class Preprocessor:
    def __init__(self):
        print("ğŸš€ [Preprocessor] åˆå§‹åŒ–è³‡æ–™è™•ç†æ¨¡çµ„...")
        self.grid_2f = self._load_map('2F_map.xlsx', 32, 61)
        self.grid_3f = self._load_map('3F_map.xlsx', 32, 61)
        self.shelf_coords = self._load_shelf_coords()
        self.inventory_map = self._load_inventory()
        
        # å»ºç«‹ç«™é»è³‡è¨Š
        self.stations = self._init_stations()
        
    def _load_map(self, filename, rows, cols):
        path = os.path.join(DATA_DIR, 'master', filename)
        if not os.path.exists(path): path = path.replace('.xlsx', '.csv')
        try:
            if filename.endswith('.xlsx'): df = pd.read_excel(path, header=None)
            else: df = pd.read_csv(path, header=None)
            raw = df.iloc[0:rows, 0:cols].fillna(0).values
            grid = np.full((rows, cols), -1.0) # -1 ä»£è¡¨ç‰†å£
            r_in, c_in = min(raw.shape[0], rows), min(raw.shape[1], cols)
            grid[0:r_in, 0:c_in] = raw[0:r_in, 0:c_in]
            return grid
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è®€å–åœ°åœ– {filename}: {e}")
            return np.full((rows, cols), 0)

    def _load_shelf_coords(self):
        path = os.path.join(DATA_DIR, 'mapping', 'shelf_coordinate_map.csv')
        coords = {}
        if os.path.exists(path):
            df = pd.read_csv(path)
            # [FIX] å¼·åˆ¶è½‰å¤§å¯«ï¼Œé¿å… Key Error
            df.columns = [c.upper().strip() for c in df.columns]
            
            # å˜—è©¦æ‰¾æ­£ç¢ºçš„æ¬„ä½å (ç›¸å®¹ä¸åŒå‘½åç¿’æ…£)
            col_shelf = next((c for c in df.columns if 'SHELF' in c), 'SHELF_ID')
            col_floor = next((c for c in df.columns if 'FLOOR' in c), 'FLOOR')
            col_x = next((c for c in df.columns if c == 'X'), 'X')
            col_y = next((c for c in df.columns if c == 'Y'), 'Y')

            for _, r in df.iterrows():
                try:
                    coords[str(r[col_shelf])] = {'floor': r[col_floor], 'pos': (int(r[col_y]), int(r[col_x]))}
                except KeyError:
                    pass # ç•¥éæ¬„ä½å°ä¸ä¸Šçš„ row
        return coords

    def _load_inventory(self):
        path = os.path.join(DATA_DIR, 'master', 'item_inventory.csv')
        inv = defaultdict(list)
        if os.path.exists(path):
            try:
                df = pd.read_csv(path, dtype=str)
                # [FIX] é—œéµä¿®æ­£ï¼šç›´æ¥ä¿®æ”¹ DataFrame çš„ columns ç‚ºå…¨å¤§å¯«
                df.columns = [c.upper().strip() for c in df.columns]
                
                cols = df.columns
                part_col = next((c for c in cols if 'PART' in c), None)
                cell_col = next((c for c in cols if 'CELL' in c or 'LOC' in c), None)
                
                if part_col and cell_col:
                    print(f"   -> Inventory æ¬„ä½å°æ‡‰: Part='{part_col}', Loc='{cell_col}'")
                    for _, r in df.iterrows():
                        val_part = r[part_col]
                        val_cell = r[cell_col]
                        if pd.notna(val_part) and pd.notna(val_cell):
                            inv[str(val_part).strip()].append(str(val_cell).strip())
                else:
                    print(f"âš ï¸ Inventory æ¬„ä½å°æ‡‰å¤±æ•—ã€‚ç¾æœ‰æ¬„ä½: {cols}")
            except Exception as e:
                print(f"âš ï¸ è®€å– Inventory å¤±æ•—: {e}")

        return inv

    def _init_stations(self):
        sts = {}
        for floor, grid in [('2F', self.grid_2f), ('3F', self.grid_3f)]:
            rows, cols = grid.shape
            cnt = 0
            for r in range(rows):
                for c in range(cols):
                    if grid[r][c] == 2: # 2 ä»£è¡¨å·¥ä½œç«™
                        cnt += 1
                        sts[f"{floor}_{cnt}"] = {'floor': floor, 'pos': (r, c)}
        return sts

    def _load_and_consolidate_orders(self):
        print("ğŸ“¦ æ­£åœ¨è®€å–ä¸¦åˆä½µè¨‚å–® (Order Batching)...")
        tasks_raw = []
        
        # 1. è®€å– Outbound
        path_out = os.path.join(DATA_DIR, 'transaction', 'wave_orders.csv')
        if os.path.exists(path_out):
            try:
                df = pd.read_csv(path_out)
                # [FIX] å¼·åˆ¶è½‰å¤§å¯«
                df.columns = [c.upper().strip() for c in df.columns]
                
                date_col = next((c for c in df.columns if 'DATETIME' == c), None)
                if not date_col:
                    date_col = next((c for c in df.columns if 'DATE' in c), None)
                
                if date_col:
                    df['datetime'] = pd.to_datetime(df[date_col])
                    df = df.dropna(subset=['datetime'])
                    if 'LOC' not in df.columns: df['LOC'] = ''
                    tasks_raw.extend(df.to_dict('records'))
                else:
                    print(f"âš ï¸ wave_orders.csv æ‰¾ä¸åˆ°æ™‚é–“æ¬„ä½ (DATETIME)")
            except Exception as e:
                print(f"âš ï¸ è®€å– wave_orders éŒ¯èª¤: {e}")
        
        # 2. è®€å– Inbound (Receiving)
        path_in = os.path.join(DATA_DIR, 'transaction', 'historical_receiving_ex.csv')
        if os.path.exists(path_in):
            try:
                df_in = pd.read_csv(path_in)
                # [FIX] å¼·åˆ¶è½‰å¤§å¯«
                df_in.columns = [c.upper().strip() for c in df_in.columns]
                
                cols = df_in.columns
                date_col = next((c for c in cols if 'DATE' in c), None)
                part_col = next((c for c in cols if 'ITEM' in c or 'PART' in c), None)
                
                if date_col and part_col:
                    df_in['datetime'] = pd.to_datetime(df_in[date_col])
                    df_in['PARTNO'] = df_in[part_col]
                    df_in['WAVE_ID'] = 'RECEIVING_' + df_in['datetime'].dt.strftime('%Y%m%d')
                    df_in['PARTCUSTID'] = 'REC_VENDOR'
                    if 'LOC' not in df_in.columns: df_in['LOC'] = ''
                    tasks_raw.extend(df_in.to_dict('records'))
            except Exception as e:
                print(f"âš ï¸ è®€å– historical_receiving éŒ¯èª¤: {e}")
        
        if not tasks_raw:
            print("âš ï¸ ç„¡ä»»ä½•è¨‚å–®è³‡æ–™ï¼")
            return {'2F': [], '3F': []}, datetime.now()

        tasks_raw.sort(key=lambda x: x['datetime'])
        base_time = tasks_raw[0]['datetime']
        
        # --- æ™ºæ…§ä½µå–® (Consolidation) ---
        print("   -> é€²è¡Œåº«å­˜åŒ¹é…èˆ‡ä½µå–®é‹ç®—...")
        part_shelf_map = {}
        valid_shelves = list(self.shelf_coords.keys())
        
        # å…ˆæƒæä¸€æ¬¡æœ‰ LOC çš„ï¼Œå»ºç«‹ PART -> LOC çš„å°æ‡‰ (é»æ»¯æ€§)
        for t in tasks_raw:
            part = str(t.get('PARTNO', '')).strip()
            loc = str(t.get('LOC', '')).strip()
            if len(loc) >= 5: # å‡è¨­è‡³å°‘è¦æœ‰é•·åº¦
                part_shelf_map[part] = loc 
        
        # å¡«è£œæ²’æœ‰ LOC çš„è¨‚å–®
        for t in tasks_raw:
            loc = str(t.get('LOC', '')).strip()
            if len(loc) < 5:
                part = str(t.get('PARTNO', '')).strip()
                if part in part_shelf_map:
                    t['LOC'] = part_shelf_map[part]
                elif part in self.inventory_map and self.inventory_map[part]:
                    chosen = self.inventory_map[part][0]
                    t['LOC'] = chosen
                    part_shelf_map[part] = chosen
                elif valid_shelves:
                    # éš¨æ©Ÿåˆ†é…ä¸€å€‹å‡ä½ç½®ï¼Œé¿å…ç•¶æ©Ÿ (æ ¼å¼: SHELF-FACE-BIN)
                    rand_shelf = random.choice(valid_shelves)
                    t['LOC'] = f"{rand_shelf}-A-01"

        # è½‰æ›ç‚º AGV ä»»å‹™æ ¼å¼
        df_tasks = pd.DataFrame(tasks_raw)
        final_queues = {'2F': [], '3F': []}
        
        if 'WAVE_ID' not in df_tasks.columns:
            df_tasks['WAVE_ID'] = 'DEFAULT_WAVE'
            
        grouped = df_tasks.groupby('WAVE_ID')
        
        st_lists = {'2F': [k for k,v in self.stations.items() if v['floor']=='2F'],
                    '3F': [k for k,v in self.stations.items() if v['floor']=='3F']}
        
        for wave_id, wave_df in grouped:
            for floor in ['2F', '3F']:
                # ç¯©é¸å±¬æ–¼è©²æ¨“å±¤çš„è¨‚å–® (ä¾æ“š LOC é–‹é ­)
                # å‡è¨­ 2F çš„ loc é–‹é ­æ˜¯ '2'ï¼Œ3F æ˜¯ '3'
                prefix = floor[0]
                f_df = wave_df[wave_df['LOC'].str.startswith(prefix, na=False)].copy()
                
                if f_df.empty: continue
                
                avail_sts = st_lists[floor]
                if not avail_sts: continue
                
                # ä¾æ“šè²¨æ¶åˆä½µä»»å‹™ (Batching by Shelf)
                shelf_tasks = defaultdict(list)
                for i, row in f_df.iterrows():
                    loc = str(row['LOC'])
                    # å‡è¨­ Shelf ID æ˜¯å‰ 9 ç¢¼ (ä¾‹å¦‚: 2F-01-01)
                    # é€™è£¡åšå€‹é˜²å‘†ï¼Œå¦‚æœé•·åº¦ä¸å¤ å°±æ•´ä¸²ç•¶ ID
                    shelf_id = loc[:9] if len(loc) >= 9 else loc
                    
                    cust_id = str(row.get('PARTCUSTID', 'UNK'))
                    target_st = avail_sts[hash(cust_id) % len(avail_sts)]
                    
                    shelf_tasks[shelf_id].append({
                        'station': target_st,
                        'qty': row.get('QTY', 1),
                        'row': row
                    })
                
                # ç”Ÿæˆæœ€çµ‚ä»»å‹™ç‰©ä»¶
                for sid, items in shelf_tasks.items():
                    target_st = items[0]['station']
                    proc_time = 15 + (len(items) * 5)
                    
                    # æ‰¾å‡ºæœ€æ—©çš„æ™‚é–“ç•¶ä½œä»»å‹™æ™‚é–“
                    min_dt = min([x['row']['datetime'] for x in items])

                    task_obj = {
                        'task_id': f"{wave_id}_{sid}",
                        'type': 'ORDER',
                        'shelf_id': sid,
                        'wave_id': wave_id,
                        'priority': 10,
                        'stops': [{'station': target_st, 'time': proc_time}],
                        'datetime': min_dt,
                        'raw_items': [x['row'] for x in items]
                    }
                    final_queues[floor].append(task_obj)
                    
        # æ’åº
        for f in final_queues:
            final_queues[f].sort(key=lambda x: x['datetime'])
            
        return final_queues, base_time

    def run(self):
        queues, base_dt = self._load_and_consolidate_orders()
        
        data = {
            'grid_2f': self.grid_2f,
            'grid_3f': self.grid_3f,
            'stations': self.stations,
            'shelf_coords': self.shelf_coords,
            'queues': queues,
            'base_time': base_dt
        }
        
        with open(OUTPUT_FILE, 'wb') as f:
            pickle.dump(data, f)
        print(f"âœ… è³‡æ–™è™•ç†å®Œæˆï¼å·²å„²å­˜è‡³ {OUTPUT_FILE}")
        print(f"   - 2F ä»»å‹™æ•¸: {len(queues['2F'])}")
        print(f"   - 3F ä»»å‹™æ•¸: {len(queues['3F'])}")

if __name__ == "__main__":
    Preprocessor().run()